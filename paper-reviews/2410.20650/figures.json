[{"figure_path": "https://arxiv.org/html/2410.20650/x1.png", "caption": "Figure 1: The histograms of different components of the parameters of LLama-3 8B model\u00a0(Dubey et\u00a0al., 2024).\nThe x\ud835\udc65xitalic_x-axis is all possible binary values and the y\ud835\udc66yitalic_y-axis represent the frequency of each value.", "description": "Figure 1 presents histograms visualizing the distribution of sign bits, exponent bits, and mantissa bits within the parameters of the LLama-3 8B model. Each histogram shows the frequency of occurrence for each possible binary value (represented on the x-axis), providing insights into the entropy of each component. This analysis is crucial to understanding NeuZip's compression strategy, which focuses on the low-entropy nature of specific bits to achieve memory efficiency.", "section": "2.1 Low-Entropy Nature of Neural Network Parameters"}, {"figure_path": "https://arxiv.org/html/2410.20650/x2.png", "caption": "(a) Vanilla", "description": "This figure illustrates the reverse-mode automatic differentiation (backpropagation) process for a linear layer in a neural network, comparing different memory-saving techniques.  (a) Vanilla shows the standard approach, where both weights and activations/gradients are stored in memory throughout the entire process.  This contrasts with methods like (b) activation checkpointing (AC), (c) AC combined with Low-Memory Optimization (LOMO), and (d) NeuZip. These optimized techniques utilize various strategies to reduce memory usage during backpropagation, either by recomputing certain values or leveraging compressed representations, as seen in NeuZip's compressed weight storage.", "section": "2.2 Lossless NeuZip: Compressing Exponents for Training"}, {"figure_path": "https://arxiv.org/html/2410.20650/x3.png", "caption": "(b) AC", "description": "This figure shows the memory usage pattern of the activation checkpointing (AC) method for a linear layer in a neural network during reverse-mode automatic differentiation (backpropagation).  Blue blocks represent data temporarily loaded into memory for calculations, while red blocks denote data constantly residing in memory.  Activation checkpointing saves memory by recomputing activations during backpropagation, but still needs to store weights and other intermediate variables. The image visually compares vanilla, AC, AC with Layer-wise Optimization using Memory Optimization (LOMO), and NeuZip, which is the proposed method in the paper.", "section": "Multi-layer neural networks"}, {"figure_path": "https://arxiv.org/html/2410.20650/x4.png", "caption": "(c) AC+LOMO", "description": "This figure shows the reverse-mode automatic differentiation (backpropagation) process for a linear layer in a neural network using the AC+LOMO memory-saving technique.  Blue blocks represent data temporarily loaded into memory during computation for the current layer, while red blocks show data that remains in memory throughout the entire training process. AC+LOMO combines activation checkpointing (AC) and Layer-wise Ordering of Memory Optimization (LOMO) to reduce memory usage.  Activation checkpointing recomputes activations instead of storing them, while LOMO optimizes memory usage by efficiently managing memory allocation and deallocation across layers.  This visualization contrasts AC+LOMO with other memory-saving approaches, highlighting its efficiency in reducing peak memory usage during training.", "section": "2.2 Lossless NeuZip: Compressing Exponents for Training"}, {"figure_path": "https://arxiv.org/html/2410.20650/x5.png", "caption": "(d) NeuZip", "description": "This figure shows a diagram illustrating the reverse-mode automatic differentiation (backpropagation) process in a linear layer of a neural network using NeuZip. It compares NeuZip's memory-saving approach with other methods like vanilla, activation checkpointing (AC), and AC combined with LOMO. Blue blocks represent data temporarily loaded into memory, while red blocks represent data persistently stored in memory. NeuZip significantly reduces memory usage by compressing weight matrices and utilizing the multi-layer structure of neural networks to avoid storing large buffers.", "section": "Multi-layer neural networks"}, {"figure_path": "https://arxiv.org/html/2410.20650/x6.png", "caption": "Figure 2: Reverse-mode automatic differentiation (e.g., back-propagation) with different memory-saving techniques for a linear layer. Blocks colored blue are loaded in memory temporarily for the calculation of this layer, whereas the blocks colored red are always in memory throughout training.", "description": "This figure illustrates the memory usage of different training methods for a single linear layer during backpropagation.  It compares vanilla training, activation checkpointing (AC), AC with Layer-wise Optimization using Memory Optimization (AC+LOMO), and the proposed NeuZip method. Blue blocks represent data loaded into memory temporarily for a single layer's computation, while red blocks denote data persistently stored throughout training. NeuZip is shown to reduce memory usage by strategically compressing parameters.", "section": "2.2 Lossless NeuZip: Compressing Exponents for Training"}, {"figure_path": "https://arxiv.org/html/2410.20650/x7.png", "caption": "Figure 3: The trade-off between memory and performance for different methods.", "description": "This figure illustrates the Pareto frontier for different model compression techniques, showing the trade-off between memory usage and model performance.  The x-axis represents memory consumption (in GiB), and the y-axis represents model performance (e.g., perplexity). Three different model sizes (Llama-3 8B, Llama-2 13B, Yi-1.5 34B) are shown, each with results for a vanilla (uncompressed) model, a quantization method, and several NeuZip variants.  Points closer to the bottom-left corner indicate better memory efficiency and higher performance.  The results demonstrate that NeuZip variants generally lie closer to or on the Pareto frontier compared to quantization methods, indicating a better balance between memory efficiency and performance.", "section": "3.4 In-Depth Analyses"}, {"figure_path": "https://arxiv.org/html/2410.20650/x8.png", "caption": "Figure 4: The throughput experiment. (a) Comparison of CPU-offloading, quantization, lossy NeuZip compression, and lossless NeuZip compression. (b) Comparison of GPU-reloading, de-quantization, lossy NeuZip decompression, and lossless NeuZip decompression.", "description": "This figure compares the throughput (in GiB/s) of various methods for compressing and decompressing matrices of varying sizes in neural network training.  Panel (a) shows the compression throughput of CPU offloading, quantization, lossy NeuZip, and lossless NeuZip. Panel (b) displays the decompression throughput of GPU reloading, de-quantization, lossy NeuZip decompression, and lossless NeuZip decompression.  The results illustrate the relative efficiency of each method in terms of data transfer rate and memory usage.", "section": "Throughputs of NeuZip"}, {"figure_path": "https://arxiv.org/html/2410.20650/x9.png", "caption": "Figure 5: The histograms of different floating-point components of the parameters of a randomly initialized Llama-3 8B model.", "description": "This figure displays histograms illustrating the distribution of sign bits, exponent bits, and mantissa bits within the floating-point numbers representing the parameters of a randomly initialized Llama-3 8B model.  The x-axis of each histogram represents the possible values for each component (bits), while the y-axis represents the frequency of occurrence for each value in the model's parameters.  The histograms visually demonstrate the low entropy nature of the exponent bits, a key observation supporting the NeuZip compression method described in the paper.", "section": "2.1 Low-Entropy Nature of Neural Network Parameters"}]