<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks &#183; AI Paper Reviews by AI</title>
<meta name=title content="NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks &#183; AI Paper Reviews by AI"><meta name=description content="NeuZip dynamically compresses neural network weights, achieving memory-efficient training and inference without performance loss, significantly reducing the memory footprint of large language models."><meta name=keywords content="Natural Language Processing,Large Language Models,üè¢ University of Alberta,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.89870b452537f28412203a1828e1f2d39a1583455d842f551c838334a33cd8b5471e8576e4a6569a8a9a67fead4f93a78857e4fb9502fd2d11471d812a6c561b.css integrity="sha512-iYcLRSU38oQSIDoYKOHy05oVg0VdhC9VHIODNKM82LVHHoV25KZWmoqaZ/6tT5OniFfk+5UC/S0RRx2BKmxWGw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/"><meta property="og:site_name" content="AI Paper Reviews by AI"><meta property="og:title" content="NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks"><meta property="og:description" content="NeuZip dynamically compresses neural network weights, achieving memory-efficient training and inference without performance loss, significantly reducing the memory footprint of large language models."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-10-28T00:00:00+00:00"><meta property="article:modified_time" content="2024-10-28T00:00:00+00:00"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Large Language Models"><meta property="article:tag" content="üè¢ University of Alberta"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/cover.png"><meta name=twitter:title content="NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks"><meta name=twitter:description content="NeuZip dynamically compresses neural network weights, achieving memory-efficient training and inference without performance loss, significantly reducing the memory footprint of large language models."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks","headline":"NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks","abstract":"NeuZip dynamically compresses neural network weights, achieving memory-efficient training and inference without performance loss, significantly reducing the memory footprint of large language models.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2410.20650\/","author":{"@type":"Person","name":"AI Paper Reviews by AI"},"copyrightYear":"2024","dateCreated":"2024-10-28T00:00:00\u002b00:00","datePublished":"2024-10-28T00:00:00\u002b00:00","dateModified":"2024-10-28T00:00:00\u002b00:00","keywords":["Natural Language Processing","Large Language Models","üè¢ University of Alberta"],"mainEntityOfPage":"true","wordCount":"2943"}]</script><meta name=author content="AI Paper Reviews by AI"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><meta name=theme-color></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">AI Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Paper Reviews</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Paper Reviews</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2410.20650/cover_hu3913457421043291517.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>AI Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2410.20650/>NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-10-28T00:00:00+00:00>28 October 2024</time><span class="px-2 text-primary-500">&#183;</span><span>2943 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">14 mins</span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/natural-language-processing/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Natural Language Processing
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/large-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Large Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-university-of-alberta/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ University of Alberta</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviews by AI" src=/ai-paper-reviewer/img/avatar_hu14127527184135390686.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviews by AI</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers in the field of AI</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#low-entropy-weights>Low-Entropy Weights</a></li><li><a href=#ans-compression>ANS Compression</a></li><li><a href=#lossy-inference>Lossy Inference</a></li><li><a href=#memory-benchmarks>Memory Benchmarks</a></li><li><a href=#future-directions>Future Directions</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#low-entropy-weights>Low-Entropy Weights</a></li><li><a href=#ans-compression>ANS Compression</a></li><li><a href=#lossy-inference>Lossy Inference</a></li><li><a href=#memory-benchmarks>Memory Benchmarks</a></li><li><a href=#future-directions>Future Directions</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2410.20650</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Yongchang Hao et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="256" height="256" viewBox="0 0 256 256" fill="none"><path d="M230.721 172.7C230.183 170.673 229.313 168.75 228.146 167.008 228.396 166.091 228.587 165.159 228.714 164.217 229.543 158.241 227.471 152.77 223.567 148.537 221.452 146.225 219.185 144.698 216.784 143.761 218.36 137.018 219.157 130.117 219.161 123.193 219.161 120.03 218.982 116.932 218.682 113.88 218.526 112.356 218.337 110.836 218.115 109.32 217.428 104.847 216.408 100.431 215.064 96.11 214.183 93.2707 213.164 90.476 212.01 87.736 210.281 83.6782 208.262 79.75 205.969 75.982 204.465 73.475 202.827 71.0508 201.062 68.72 200.197 67.543 199.296 66.3938 198.358 65.274 195.58 61.898 192.561 58.7277 189.325 55.788 188.25 54.7997 187.145 53.8453 186.01 52.926 184.893 51.9943 183.751 51.0927 182.586 50.222c-2.345-1.7454-4.768-3.3828-7.262-4.907-13.781-8.37-29.942-13.17-47.215-13.17-50.292.0-91.052 40.762-91.052 91.051C37.055 130.208 37.867 137.196 39.477 144.02 37.317 144.958 35.247 146.42 33.327 148.535 29.424 152.766 27.351 158.217 28.18 164.193 28.306 165.142 28.495 166.082 28.747 167.006 27.5811 168.749 26.7117 170.673 26.174 172.7 24.974 177.261 25.369 181.374 26.894 184.978 25.236 189.688 25.65 194.704 27.809 199.065c1.57 3.185 3.817 5.649 6.587 7.851C37.689 209.534 41.811 211.758 46.783 213.892 52.715 216.422 59.956 218.799 63.249 219.671 71.755 221.873 79.911 223.269 88.177 223.337 99.954 223.446 110.096 220.677 117.357 213.59 120.924 214.027 124.515 214.246 128.109 214.244 131.906 214.236 135.699 213.997 139.467 213.529 146.711 220.661 156.892 223.455 168.712 223.343 176.977 223.277 185.133 221.881 193.617 219.676 196.932 218.804 204.17 216.427 210.105 213.897 215.077 211.76 219.199 209.536 222.514 206.922c2.749-2.203 4.994-4.666 6.565-7.851C231.26 194.709 231.652 189.693 230.017 184.983 231.527 181.379 231.92 177.257 230.721 172.7zm-8.44 11.973C223.952 187.844 224.059 191.427 222.585 194.764 220.349 199.821 214.795 203.805 204.008 208.082 197.3 210.742 191.158 212.443 191.104 212.458 182.232 214.759 174.208 215.928 167.262 215.928 155.76 215.928 147.201 212.754 141.773 206.486 132.594 208.05 123.222 208.103 114.026 206.644 108.591 212.808 100.081 215.928 88.676 215.928c-6.947.0-14.97-1.16900000000001-23.843-3.47C64.779 212.443 58.639 210.742 51.929 208.082 41.143 203.805 35.587 199.824 33.352 194.764 31.878 191.427 31.985 187.844 33.656 184.673 33.81 184.378 33.976 184.091 34.153 183.813 33.1516 182.309 32.4799 180.61 32.182 178.827 31.8842 177.045 31.967 175.22 32.425 173.472 33.089 170.949 34.46 168.851 36.322 167.344 35.425 165.87 34.8365 164.23 34.592 162.522 34.056 158.808 35.289 155.1 38.062 152.076 40.222 149.723 43.275 148.428 46.655 148.428H46.745C44.1965 140.259 42.9044 131.75 42.913 123.193c0-46.671 37.836-84.51 84.514-84.51 46.677.0 84.513 37.835 84.513 84.51C211.947 131.773 210.646 140.304 208.081 148.492 208.489 148.452 208.889 148.432 209.282 148.431 212.662 148.431 215.716 149.726 217.874 152.079 220.647 155.1 221.881 158.811 221.344 162.525 221.1 164.233 220.511 165.873 219.615 167.347 221.477 168.854 222.849 170.952 223.512 173.475 223.97 175.223 224.053 177.048 223.755 178.831 223.458 180.613 222.786 182.312 221.784 183.816 221.961 184.091 222.129 184.378 222.281 184.673z" fill="#fff"/><path d="M221.784 183.816C222.786 182.312 223.458 180.613 223.756 178.831 224.053 177.048 223.97 175.223 223.512 173.475 222.848 170.952 221.476 168.854 219.615 167.347 220.512 165.873 221.1 164.233 221.344 162.525 221.881 158.811 220.648 155.103 217.874 152.079 215.716 149.726 212.662 148.431 209.282 148.431 208.889 148.431 208.489 148.452 208.081 148.492 210.643 140.304 211.942 131.774 211.933 123.195c0-46.6719-37.836-84.5099-84.509-84.5099-46.674.0-84.5141 37.834-84.5141 84.5099C42.9015 131.752 44.1936 140.261 46.742 148.43H46.6519C43.2719 148.43 40.219 149.724 38.06 152.077 35.287 155.098 34.0529 158.81 34.5899 162.523 34.8346 164.231 35.4231 165.872 36.3199 167.346 34.4579 168.852 33.086 170.95 32.422 173.473 31.9642 175.222 31.8817 177.047 32.1799 178.83 32.4781 180.612 33.1501 182.312 34.1519 183.816 33.9739 184.094 33.8099 184.381 33.6549 184.676 31.9849 187.847 31.877 191.43 33.352 194.767 35.588 199.824 41.1419 203.808 51.9289 208.085 58.6359 210.745 64.779 212.446 64.833 212.461 73.705 214.762 81.729 215.931 88.675 215.931c11.406.0 19.916-3.12 25.351-9.28400000000002C123.222 208.106 132.594 208.052 141.773 206.489 147.201 212.757 155.76 215.931 167.262 215.931c6.946.0 14.97-1.16900000000001 23.841-3.47C191.158 212.446 197.298 210.745 204.008 208.085 214.795 203.808 220.35 199.824 222.585 194.767 224.059 191.43 223.952 187.847 222.281 184.676 222.129 184.379 221.961 184.091 221.784 183.816zM110.137 196.997C109.669 197.815 109.168 198.614 108.635 199.391 107.23 201.448 105.382 203.02 103.237 204.188 99.1369 206.424 93.947 207.205 88.675 207.205 80.346 207.205 71.808 205.256 67.023 204.015 66.787 203.954 37.689 195.735 41.373 188.739 41.993 187.562 43.0129 187.092 44.2979 187.092 49.4849 187.092 58.9299 194.816 62.9889 194.816 63.8959 194.816 64.5359 194.43 64.7969 193.488 66.5269 187.284 38.5039 184.676 40.8639 175.692 41.2799 174.102 42.41 173.456 43.998 173.456 50.856 173.455 66.248 185.516 69.467 185.516 69.714 185.516 69.8909 185.443 69.9869 185.291 70.0009 185.268 70.015 185.246 70.028 185.222 71.539 182.727 70.6719 180.913 60.3209 174.573L59.3269 173.968C47.9359 167.074 39.9409 162.925 44.4879 157.975 45.0109 157.404 45.7529 157.151 46.6539 157.151 47.7219 157.151 49.0149 157.508 50.4389 158.108 56.4549 160.645 64.793 167.564 68.276 170.581 68.8239 171.057 69.3683 171.538 69.9089 172.022 69.9089 172.022 74.319 176.608 76.985 176.608 77.599 176.608 78.1199 176.366 78.4729 175.768 80.364 172.58 60.9099 157.838 59.8129 151.755 59.0689 147.634 60.3349 145.546 62.6749 145.546 63.7879 145.546 65.1459 146.02 66.6449 146.971 71.2949 149.922 80.2729 165.35 83.5599 171.352 84.6619 173.363 86.5429 174.213 88.2379 174.213 91.6009 174.213 94.2299 170.87 88.5459 166.622 80.0029 160.23 83.001 149.782 87.078 149.139 87.252 149.111 87.4279 149.097 87.6029 149.097 91.3109 149.097 92.9459 155.486 92.9459 155.486S97.7399 167.524 105.975 175.753C113.447 183.222 114.491 189.351 110.137 196.997zm26.629 1.41L136.339 198.458 135.611 198.541C135.228 198.581 134.844 198.619 134.459 198.654L134.084 198.688 133.741 198.717 133.255 198.756 132.718 198.795 132.182 198.83 132.063 198.838C131.923 198.846 131.783 198.855 131.641 198.862L131.462 198.872C131.296 198.881 131.13 198.889 130.962 198.896L130.381 198.921 129.854 198.939 129.502 198.949H129.323C129.213 198.949 129.104 198.955 128.994 198.956H128.82C128.71 198.956 128.601 198.956 128.491 198.961L128.043 198.967h-.625C126.927 198.967 126.437 198.962 125.949 198.952L125.553 198.943C125.44 198.943 125.327 198.938 125.216 198.934L124.796 198.922 124.275 198.902 123.805 198.881 123.684 198.876 123.237 198.853C123.112 198.846 122.989 198.84 122.865 198.831L122.576 198.814C122.213 198.791 121.85 198.766 121.487 198.738L121.107 198.707C120.947 198.695 120.787 198.68 120.628 198.666 120.441 198.65 120.254 198.632 120.067 198.614 119.754 198.585 119.441 198.553 119.128 198.519H119.113C123.683 188.324 121.372 178.802 112.137 169.575 106.08 163.526 102.051 154.594 101.215 152.633 99.5229 146.828 95.045 140.375 87.608 140.375 86.979 140.375 86.351 140.425 85.73 140.523 82.472 141.036 79.624 142.911 77.592 145.733 75.396 143.002 73.262 140.831 71.332 139.605 68.422 137.76 65.5179 136.824 62.6889 136.824 59.1579 136.824 56.0019 138.274 53.8019 140.904L53.7459 140.971C53.7039 140.798 53.6639 140.625 53.6229 140.451L53.6179 140.428C53.1992 138.638 52.8477 136.833 52.5639 135.016 52.5639 135.004 52.5639 134.992 52.5579 134.98 52.5359 134.843 52.5159 134.705 52.4949 134.568 52.4334 134.162 52.3757 133.755 52.3219 133.348 52.2979 133.163 52.2719 132.978 52.2489 132.793L52.1809 132.238C52.1589 132.053 52.1409 131.885 52.1209 131.709L52.115 131.665C52.0351 130.945 51.9651 130.225 51.9049 129.503L51.8829 129.226 51.8479 128.754C51.8379 128.625 51.8279 128.495 51.8209 128.365 51.8209 128.334 51.8159 128.304 51.8149 128.275 51.7895 127.913 51.7678 127.55 51.7499 127.187 51.7399 126.998 51.7299 126.81 51.7219 126.62L51.7019 126.124 51.6969 125.974 51.6809 125.517 51.6709 125.128C51.6709 124.973 51.6629 124.818 51.6609 124.663 51.6579 124.508 51.6539 124.338 51.6529 124.174 51.6509 124.01 51.6529 123.848 51.6479 123.685 51.6439 123.521 51.6479 123.358 51.6479 123.195c0-41.8529 33.931-75.7839 75.7881-75.7839 41.856.0 75.786 33.93 75.786 75.7839V124.174C203.222 124.337 203.217 124.501 203.214 124.663 203.214 124.798 203.208 124.931 203.204 125.068 203.204 125.188 203.199 125.309 203.195 125.425 203.195 125.578 203.186 125.731 203.181 125.884V125.896L203.16 126.427C203.153 126.582 203.147 126.738 203.139 126.893L203.134 127.003 203.107 127.499C203.048 128.562 202.967 129.623 202.866 130.683V130.696C202.849 130.87 202.832 131.044 202.813 131.218L202.768 131.629 202.679 132.433 202.628 132.84 202.565 133.319C202.542 133.493 202.519 133.668 202.493 133.841 202.467 134.036 202.438 134.23 202.409 134.424L202.34 134.883 202.258 135.403C202.23 135.576 202.2 135.748 202.168 135.92 202.135 136.093 202.109 136.265 202.079 136.437 202.019 136.781 201.956 137.125 201.89 137.468 201.789 137.981 201.686 138.493 201.58 139.005L201.47 139.512C201.434 139.681 201.395 139.851 201.357 140.02 199.224 137.947 196.399 136.818 193.284 136.818 190.457 136.818 187.55 137.753 184.641 139.598 182.711 140.824 180.578 142.996 178.381 145.726 176.346 142.904 173.498 141.029 170.242 140.516 169.621 140.418 168.993 140.368 168.364 140.368 160.925 140.368 156.45 146.821 154.757 152.626 153.917 154.587 149.887 163.519 143.825 169.577 134.596 178.775 132.268 188.254 136.766 198.407zm78.241-20.409L214.977 178.087C214.901 178.288 214.813 178.484 214.714 178.674 214.639 178.814 214.558 178.95 214.47 179.082 214.303 179.331 214.12 179.569 213.921 179.793 213.875 179.845 213.831 179.897 213.779 179.948 213.707 180.025 213.634 180.101 213.559 180.175 212.213 181.509 210.161 182.679 207.841 183.752 207.578 183.871 207.311 183.99 207.042 184.11L206.774 184.229C206.595 184.308 206.416 184.386 206.228 184.463 206.049 184.541 205.863 184.619 205.677 184.695L205.119 184.925C203.814 185.462 202.477 185.974 201.173 186.479L200.615 186.696 200.064 186.912C199.697 187.055 199.335 187.198 198.979 187.341L198.448 187.555 197.926 187.768 197.67 187.876C197.499 187.947 197.332 188.018 197.165 188.089 193.328 189.736 190.567 191.411 191.147 193.489 191.163 193.548 191.181 193.604 191.201 193.659 191.253 193.813 191.324 193.958 191.413 194.095 191.465 194.176 191.525 194.253 191.592 194.323 192.274 195.032 193.515 194.92 195.08 194.357 195.3 194.276 195.519 194.192 195.736 194.104L195.872 194.048C196.23 193.896 196.609 193.726 196.996 193.542 197.093 193.496 197.191 193.452 197.289 193.401 199.203 192.465 201.372 191.205 203.524 190.058 204.385 189.593 205.258 189.152 206.142 188.733 208.18 187.774 210.096 187.094 211.636 187.094 212.359 187.094 212.997 187.242 213.529 187.582L213.618 187.641C213.952 187.876 214.232 188.178 214.441 188.528 214.482 188.595 214.522 188.666 214.561 188.739 215.322 190.184 214.685 191.68 213.194 193.147 211.763 194.556 209.537 195.937 207.007 197.215 206.819 197.31 206.631 197.405 206.44 197.498 198.91 201.196 189.049 203.981 188.912 204.016 186.284 204.697 182.526 205.591 178.292 206.26L177.666 206.358 177.563 206.373C177.089 206.445 176.614 206.512 176.138 206.574 175.655 206.639 175.167 206.698 174.676 206.753L174.586 206.763C172.806 206.968 171.019 207.104 169.228 207.169H169.202C168.554 207.192 167.907 207.204 167.259 207.204H166.512C165.524 207.191 164.538 207.146 163.553 207.07 163.53 207.07 163.505 207.07 163.482 207.064 163.129 207.037 162.777 207.004 162.425 206.965 162.06 206.926 161.696 206.882 161.333 206.833 161.094 206.801 160.856 206.765 160.618 206.726 160.376 206.687 160.134 206.647 159.893 206.605L159.564 206.543 159.539 206.538C159.192 206.472 158.847 206.399 158.503 206.319 158.303 206.274 158.104 206.23 157.907 206.176L157.788 206.146C157.69 206.122 157.595 206.096 157.498 206.07L157.445 206.056 157.137 205.966C157.025 205.935 156.913 205.901 156.801 205.868L156.762 205.857 156.471 205.768C156.361 205.734 156.251 205.698 156.142 205.662L155.874 205.573 155.677 205.504C155.487 205.437 155.298 205.368 155.111 205.296L154.933 205.226 154.786 205.168C154.502 205.054 154.22 204.935 153.941 204.81L153.756 204.72 153.725 204.706C153.659 204.675 153.594 204.644 153.528 204.617 153.399 204.555 153.271 204.491 153.144 204.426L153.105 204.407 152.921 204.31C152.594 204.139 152.274 203.957 151.96 203.764L151.788 203.658C151.702 203.605 151.616 203.55 151.532 203.494L151.308 203.346 151.067 203.18 150.923 203.077C150.771 202.969 150.622 202.857 150.476 202.742L150.243 202.563C150.15 202.488 150.058 202.412 149.967 202.335 149.89 202.272 149.815 202.206 149.74 202.14L149.734 202.135C149.653 202.064 149.574 201.993 149.495 201.92 149.417 201.849 149.339 201.777 149.263 201.704L149.254 201.695C149.174 201.619 149.096 201.542 149.019 201.463 148.942 201.385 148.863 201.307 148.788 201.227 148.713 201.148 148.636 201.067 148.562 200.984 148.488 200.902 148.42 200.827 148.35 200.746L148.327 200.719C148.259 200.641 148.192 200.562 148.126 200.481 147.983 200.31 147.844 200.135 147.71 199.956 147.575 199.776 147.443 199.592 147.314 199.405L147.191 199.221C147.027 198.981 146.867 198.739 146.712 198.493 146.596 198.316 146.483 198.138 146.373 197.957 146.302 197.844 146.234 197.73 146.166 197.618L146.138 197.572C146.073 197.462 146.009 197.354 145.947 197.245 145.911 197.186 145.877 197.127 145.845 197.066 145.812 197.004 145.774 196.941 145.739 196.878L145.682 196.779 145.647 196.715C145.58 196.595 145.514 196.474 145.45 196.352 145.42 196.298 145.391 196.244 145.36 196.192L145.271 196.019 145.181 195.848C144.956 195.398 144.743 194.942 144.543 194.48L144.472 194.311C144.426 194.198 144.383 194.086 144.337 193.975 144.315 193.921 144.293 193.868 144.274 193.814 144.167 193.537 144.067 193.257 143.975 192.975 143.942 192.874 143.91 192.775 143.88 192.675 143.808 192.448 143.743 192.219 143.685 191.988 143.614 191.719 143.551 191.448 143.498 191.175 143.487 191.12 143.476 191.065 143.467 191.012 143.415 190.745 143.373 190.476 143.34 190.206 143.332 190.153 143.326 190.1 143.32 190.047L143.303 189.885C143.281 189.673 143.264 189.46 143.254 189.247 143.254 189.193 143.249 189.139 143.247 189.087 143.242 188.981 143.24 188.875 143.239 188.769 143.183 184.496 145.345 180.388 149.968 175.767 158.203 167.54 162.997 155.501 162.997 155.501S163.126 154.996 163.394 154.269C163.431 154.168 163.47 154.064 163.514 153.955 163.67 153.548 163.846 153.148 164.041 152.758L164.08 152.683C164.246 152.351 164.428 152.027 164.624 151.712 164.67 151.639 164.714 151.567 164.765 151.494 164.912 151.277 165.067 151.065 165.23 150.86 165.319 150.749 165.416 150.639 165.513 150.532 165.552 150.49 165.59 150.448 165.631 150.408 166.108 149.915 166.653 149.513 167.27 149.299L167.348 149.273C167.4 149.256 167.452 149.24 167.505 149.225 167.566 149.209 167.627 149.195 167.69 149.182L167.719 149.176C167.849 149.15 167.981 149.133 168.114 149.124H168.125C168.194 149.124 168.264 149.117 168.335 149.117 168.424 149.117 168.507 149.117 168.594 149.126 168.684 149.134 168.773 149.144 168.863 149.158 169.605 149.276 170.311 149.718 170.919 150.4 171.15 150.66 171.358 150.94 171.54 151.236 171.66 151.428 171.773 151.631 171.88 151.845 171.923 151.934 171.964 152.016 172.004 152.104 172.108 152.33 172.202 152.56 172.284 152.795 172.479 153.345 172.626 153.911 172.723 154.487 172.807 154.992 172.857 155.502 172.873 156.013 172.881 156.286 172.881 156.563 172.873 156.842 172.819 158.14 172.553 159.421 172.086 160.634 172.044 160.745 171.997 160.857 171.952 160.969 171.86 161.195 171.759 161.417 171.65 161.634 171.569 161.799 171.484 161.965 171.392 162.13 171.332 162.24 171.269 162.35 171.206 162.46 171.045 162.734 170.871 163.006 170.684 163.277L170.571 163.439C170.129 164.055 169.637 164.633 169.099 165.167 168.569 165.698 168.001 166.189 167.4 166.637 166.798 167.083 166.233 167.577 165.711 168.114 164.208 169.691 163.858 171.083 164.196 172.138 164.25 172.304 164.321 172.465 164.407 172.617 164.508 172.791 164.628 172.951 164.764 173.097L164.817 173.152 164.871 173.206C164.925 173.258 164.982 173.309 165.043 173.359L165.103 173.407C165.248 173.519 165.402 173.619 165.563 173.707 165.61 173.732 165.652 173.757 165.705 173.781 165.879 173.866 166.058 173.939 166.242 173.998 166.293 174.015 166.344 174.03 166.396 174.046L166.461 174.063 166.551 174.087 166.628 174.106 166.712 174.124 166.795 174.141 166.874 174.154C166.932 174.164 166.992 174.174 167.052 174.181L167.109 174.19 167.213 174.2 167.277 174.207 167.382 174.214H167.444L167.554 174.22H167.9L167.999 174.214 168.113 174.207 168.252 174.194 168.382 174.179C168.412 174.179 168.442 174.171 168.472 174.165 168.872 174.107 169.264 174.001 169.639 173.849L169.798 173.782C169.887 173.743 169.977 173.702 170.059 173.658 170.235 173.57 170.406 173.47 170.57 173.361 170.799 173.211 171.015 173.043 171.217 172.858 171.265 172.815 171.312 172.769 171.358 172.725 171.381 172.703 171.403 172.682 171.425 172.658 171.469 172.613 171.514 172.569 171.558 172.52 171.878 172.168 172.155 171.78 172.383 171.363 174.34 167.804 176.391 164.298 178.534 160.849L178.828 160.378 179.125 159.907C179.273 159.668 179.423 159.433 179.572 159.199L179.722 158.965C180.22 158.185 180.726 157.41 181.241 156.641L181.546 156.185C182.158 155.278 182.768 154.396 183.373 153.558L183.674 153.143C184.332 152.236 185.017 151.348 185.728 150.482L186.01 150.144C186.057 150.088 186.1 150.032 186.151 149.978 186.244 149.868 186.337 149.761 186.428 149.657 186.474 149.604 186.517 149.552 186.566 149.5L186.834 149.198 186.968 149.051C187.103 148.906 187.235 148.767 187.365 148.634 187.455 148.544 187.538 148.455 187.624 148.371 188.131 147.853 188.69 147.388 189.293 146.985L189.433 146.895C189.567 146.805 189.706 146.721 189.848 146.645 192.212 145.303 194.169 145.204 195.296 146.331 195.978 147.013 196.356 148.144 196.335 149.718 196.335 149.787 196.335 149.857 196.33 149.929V150.006C196.33 150.078 196.324 150.15 196.318 150.223 196.318 150.313 196.308 150.402 196.299 150.492 196.29 150.581 196.285 150.649 196.276 150.729 196.276 150.751 196.272 150.774 196.268 150.798 196.262 150.867 196.253 150.938 196.243 151.009 196.243 151.03 196.243 151.052 196.235 151.074 196.224 151.169 196.21 151.263 196.194 151.357 196.183 151.447 196.168 151.531 196.152 151.619L196.126 151.768C196.1 151.91 196.067 152.05 196.026 152.188 195.948 152.447 195.854 152.7 195.743 152.946 195.588 153.284 195.417 153.613 195.229 153.933 195.125 154.111 195.018 154.286 194.907 154.459 194.793 154.638 194.673 154.819 194.549 155.002 194.233 155.454 193.905 155.897 193.564 156.33L193.408 156.527C192.852 157.22 192.278 157.899 191.686 158.562L191.499 158.772C191.247 159.053 190.991 159.336 190.729 159.62L190.532 159.834C190.401 159.977 190.264 160.12 190.132 160.264 190.001 160.407 189.864 160.552 189.726 160.697L189.315 161.13 188.898 161.566 188.478 162.002C188.196 162.294 187.913 162.586 187.628 162.878 183.573 167.037 179.301 171.182 177.855 173.766 177.758 173.934 177.671 174.108 177.593 174.285 177.387 174.755 177.301 175.157 177.36 175.482 177.379 175.589 177.416 175.691 177.471 175.785 177.552 175.926 177.651 176.056 177.766 176.172 177.819 176.224 177.875 176.272 177.934 176.316 178.232 176.528 178.591 176.637 178.957 176.627H179.071L179.188 176.618 179.305 176.605 179.402 176.591C179.415 176.589 179.429 176.587 179.442 176.583L179.531 176.566 179.554 176.561 179.653 176.54 179.688 176.531C179.723 176.522 179.757 176.513 179.792 176.503 179.827 176.493 179.875 176.48 179.917 176.466 180.093 176.413 180.265 176.35 180.434 176.278 180.523 176.242 180.61 176.203 180.696 176.161 180.741 176.141 180.786 176.12 180.828 176.098L180.962 176.032C181.282 175.866 181.594 175.685 181.898 175.491L182.031 175.401C182.076 175.373 182.121 175.344 182.164 175.312L182.297 175.223 182.368 175.174 182.56 175.039C182.739 174.916 182.906 174.789 183.075 174.66L183.09 174.648 183.359 174.44C183.726 174.15 184.074 173.858 184.39 173.583L184.6 173.399 184.619 173.381 184.729 173.284C184.987 173.052 185.217 172.836 185.408 172.658L185.487 172.581C185.556 172.516 185.619 172.455 185.676 172.403L185.788 172.292 185.828 172.253 185.839 172.242 185.956 172.125 186.03 172.048 186.039 172.041 186.074 172.009 186.118 171.969 186.132 171.956 186.169 171.922 186.373 171.743 186.487 171.641C186.548 171.588 186.607 171.534 186.666 171.479L186.802 171.358C186.827 171.338 186.851 171.316 186.876 171.294L187.019 171.169 187.229 170.984 187.341 170.887C187.776 170.509 188.305 170.052 188.913 169.537L189.162 169.326 189.573 168.981 189.994 168.63C190.544 168.173 191.136 167.688 191.762 167.185L192.173 166.855C192.523 166.576 192.882 166.292 193.246 166.006 193.393 165.891 193.542 165.776 193.694 165.662 194.066 165.373 194.44 165.086 194.817 164.803 195.675 164.155 196.56 163.506 197.456 162.874L197.84 162.606C198.109 162.421 198.377 162.235 198.645 162.054L198.888 161.89C199.367 161.565 199.853 161.248 200.343 160.939L200.586 160.786 200.827 160.636C201.069 160.486 201.309 160.339 201.548 160.196L201.787 160.053 202.265 159.775 202.734 159.506 202.829 159.454 203.2 159.25C203.355 159.166 203.509 159.085 203.663 159.006L203.892 158.888 204.115 158.776C204.193 158.739 204.27 158.7 204.346 158.663 204.848 158.415 205.36 158.187 205.88 157.979 206.021 157.919 206.161 157.865 206.3 157.818L206.71 157.674C206.833 157.633 206.953 157.594 207.068 157.559L207.108 157.547C207.17 157.527 207.232 157.509 207.293 157.493L207.311 157.488C207.439 157.451 207.566 157.419 207.691 157.389H207.7C208.054 157.304 208.414 157.243 208.777 157.206 208.944 157.189 209.111 157.18 209.279 157.181H209.363C209.475 157.181 209.583 157.188 209.69 157.199 209.739 157.199 209.788 157.209 209.836 157.215H209.856C209.904 157.221 209.952 157.228 210 157.239 210.047 157.248 210.095 157.256 210.141 157.267H210.156C210.203 157.277 210.245 157.289 210.294 157.303 210.548 157.374 210.79 157.484 211.012 157.628 211.121 157.699 211.223 157.779 211.317 157.868L211.344 157.894C211.362 157.91 211.379 157.927 211.395 157.944L211.444 157.997C211.846 158.418 212.178 158.901 212.428 159.427L212.466 159.517C212.551 159.717 212.618 159.924 212.666 160.135 212.808 160.781 212.753 161.455 212.508 162.07 212.415 162.318 212.302 162.557 212.169 162.785 211.858 163.309 211.489 163.796 211.07 164.237L210.981 164.332C210.848 164.472 210.71 164.612 210.565 164.752 210.501 164.815 210.434 164.877 210.367 164.94L210.162 165.129 210.055 165.224C209.797 165.454 209.532 165.677 209.263 165.893 209.1 166.025 208.936 166.154 208.77 166.281 208.184 166.729 207.587 167.161 206.979 167.578 206.612 167.83 206.242 168.077 205.869 168.321 204.95 168.924 204.021 169.512 203.083 170.084 201.115 171.294 198.934 172.588 196.609 173.995L196.007 174.36C195.348 174.762 194.726 175.146 194.14 175.512L193.845 175.697 193.287 176.055C192.917 176.292 192.548 176.531 192.179 176.77L191.882 176.966C191.737 177.06 191.593 177.156 191.449 177.252L191.308 177.342 190.876 177.633 190.647 177.79 190.379 177.976 190.13 178.149C189.713 178.444 189.325 178.725 188.968 178.992L188.834 179.094C188.624 179.253 188.416 179.415 188.211 179.58 187.902 179.829 187.62 180.067 187.367 180.296L187.243 180.409C187.172 180.474 187.102 180.539 187.035 180.603 186.989 180.648 186.946 180.693 186.898 180.736L186.834 180.8C186.691 180.944 186.551 181.091 186.416 181.242L186.35 181.318C186.203 181.488 186.075 181.651 185.963 181.81L185.913 181.881C185.825 182.009 185.744 182.141 185.671 182.277 185.652 182.311 185.635 182.345 185.618 182.379L185.569 182.481 185.536 182.555 185.515 182.605 185.498 182.65 185.475 182.711C185.413 182.88 185.37 183.056 185.345 183.234L185.337 183.296 185.331 183.354V183.669C185.331 183.695 185.331 183.721 185.338 183.749L185.343 183.797C185.343 183.823 185.349 183.848 185.353 183.876 185.357 183.902 185.364 183.949 185.372 183.986V183.991C185.379 184.026 185.386 184.06 185.395 184.095 185.404 184.13 185.413 184.17 185.424 184.206 185.443 184.277 185.467 184.347 185.492 184.417 185.508 184.459 185.523 184.5 185.54 184.541 185.54 184.549 185.546 184.558 185.55 184.566L185.586 184.647 185.636 184.758C185.69 184.873 185.749 184.985 185.813 185.094L185.879 185.208 185.947 185.322C185.959 185.341 185.973 185.359 185.988 185.376L186.01 185.399 186.035 185.422 186.061 185.442C186.099 185.469 186.14 185.49 186.183 185.505 186.206 185.513 186.23 185.519 186.254 185.525 186.831 185.655 188.017 185.178 189.593 184.346 189.682 184.298 189.78 184.248 189.875 184.196L190.355 183.934 190.589 183.804C190.756 183.715 190.926 183.614 191.1 183.515L191.417 183.336C193.5 182.137 195.988 180.597 198.56 179.093 198.801 178.952 199.043 178.811 199.285 178.672L199.771 178.361C200.335 178.038 200.902 177.719 201.471 177.404 202.188 177.01 202.91 176.626 203.639 176.254L204.115 176.013C204.431 175.857 204.744 175.705 205.053 175.557 205.651 175.273 206.256 175.003 206.868 174.748L207.203 174.612 207.243 174.596C209.018 173.893 210.627 173.459 211.929 173.459 212.21 173.456 212.492 173.48 212.769 173.528H212.778C212.867 173.544 212.948 173.562 213.031 173.582H213.046C213.259 173.636 213.466 173.713 213.662 173.812 213.937 173.954 214.184 174.143 214.393 174.371 214.489 174.477 214.574 174.592 214.649 174.714 214.789 174.929 214.899 175.162 214.978 175.406 215.01 175.501 215.038 175.594 215.067 175.693 215.278 176.45 215.257 177.253 215.007 177.998z" fill="#ff9d00"/><path fill-rule="evenodd" clip-rule="evenodd" d="M203.21 123.685v-.491c0-41.854-33.918-75.783-75.775-75.783-41.8559.0-75.787 33.931-75.787 75.783V123.358C51.646 123.467 51.645 123.576 51.648 123.685 51.6529 123.848 51.6546 124.011 51.653 124.174L51.6581 124.534 51.661 124.663C51.661 124.723 51.6631 124.782 51.6651 124.842 51.6681 124.937 51.67 125.033 51.67 125.128L51.681 125.517 51.697 125.974 51.702 126.124 51.722 126.597V126.62C51.73 126.805 51.7401 126.989 51.7491 127.173L51.75 127.187C51.76 127.375 51.7701 127.564 51.7821 127.753 51.7921 127.927 51.802 128.101 51.815 128.275L51.8171 128.306C51.8258 128.455 51.8358 128.605 51.847 128.754L51.85 128.794 51.883 129.226 51.8861 129.254C51.8921 129.338 51.898 129.422 51.906 129.503 51.9658 130.224 52.0355 130.945 52.1151 131.664L52.12 131.709 52.181 132.238 52.2491 132.793 52.299 133.17 52.322 133.347C52.3753 133.755 52.433 134.162 52.495 134.568L52.4991 134.595 52.558 134.979C52.8435 136.808 53.1971 138.626 53.618 140.429L53.6231 140.451 53.655 140.586 53.746 140.971 53.802 140.904C56.002 138.274 59.158 136.824 62.689 136.824 65.519 136.824 68.4221 137.76 71.3321 139.605 73.2621 140.831 75.3961 143.002 77.5921 145.733 79.6241 142.911 82.4721 141.035 85.7301 140.523 86.3513 140.425 86.9792 140.376 87.6081 140.375 95.0441 140.375 99.523 146.828 101.215 152.633 102.051 154.594 106.08 163.526 112.156 169.568 121.392 178.795 123.703 188.316 119.132 198.511H119.148C119.459 198.546 119.772 198.578 120.087 198.607 120.274 198.625 120.46 198.643 120.648 198.659L120.714 198.665 121.127 198.7 121.507 198.73C121.869 198.758 122.232 198.784 122.596 198.807L122.885 198.824 123.114 198.838 123.256 198.846 123.703 198.869 123.825 198.874 124.294 198.895 124.816 198.915 125.235 198.927 125.305 198.929C125.394 198.933 125.483 198.936 125.572 198.936L125.668 198.939C126.258 198.953 126.847 198.96 127.437 198.959H128.063L128.51 198.954C128.62 198.949 128.729 198.949 128.84 198.949H129.014L129.165 198.945C129.224 198.943 129.283 198.941 129.343 198.941H129.522L129.873 198.932 130.401 198.914 130.982 198.888C131.15 198.882 131.316 198.873 131.482 198.865L131.661 198.854 131.927 198.84 132.083 198.831 132.201 198.823 132.738 198.788 133.274 198.749 133.761 198.71 134.103 198.681 134.479 198.647C135.107 198.591 135.733 198.525 136.359 198.45L136.786 198.399C132.287 188.247 134.616 178.767 143.813 169.577 149.876 163.519 153.905 154.587 154.745 152.625 156.438 146.821 160.914 140.368 168.352 140.368 168.981 140.368 169.61 140.418 170.231 140.516 173.486 141.028 176.334 142.904 178.369 145.726 180.566 142.996 182.699 140.823 184.63 139.597 187.539 137.753 190.445 136.817 193.272 136.817 196.388 136.817 199.212 137.947 201.345 140.02 201.384 139.851 201.422 139.682 201.459 139.512L201.568 139.006C201.607 138.821 201.646 138.636 201.683 138.451 201.749 138.124 201.815 137.797 201.878 137.467 201.944 137.125 202.007 136.781 202.067 136.437L202.098 136.251C202.117 136.141 202.135 136.031 202.156 135.92 202.19 135.748 202.218 135.576 202.246 135.402L202.257 135.336 202.328 134.883 202.398 134.424V134.42C202.449 134.081 202.497 133.742 202.542 133.403L202.553 133.319 202.616 132.841 202.667 132.433 202.757 131.629 202.792 131.306 202.801 131.218C202.82 131.044 202.838 130.87 202.854 130.696V130.682C202.867 130.544 202.881 130.405 202.893 130.266 202.964 129.478 203.024 128.686 203.072 127.891 203.081 127.761 203.088 127.63 203.096 127.499V127.493L203.122 127.002 203.128 126.892C203.144 126.56 203.158 126.228 203.169 125.896V125.884L203.174 125.754C203.179 125.645 203.183 125.535 203.183 125.425L203.185 125.381C203.189 125.278 203.193 125.172 203.193 125.067L203.196 124.977C203.199 124.872 203.202 124.768 203.202 124.663L203.204 124.574C203.207 124.441 203.21 124.307 203.21 124.174V123.685zm-94.572 75.706C114.64 190.59 114.214 183.984 105.98 175.754c-8.2359-8.231-13.029-20.267-13.029-20.267S91.1621 148.496 87.0821 149.138 80.0091 160.227 88.5521 166.622C97.0941 173.017 86.8521 177.353 83.5641 171.352 80.2761 165.35 71.299 149.923 66.645 146.972 61.991 144.021 58.718 145.675 59.815 151.757 60.36 154.776 65.4281 159.929 70.1631 164.743 74.9671 169.627 79.428 174.163 78.474 175.768 76.581 178.955 69.9141 172.023 69.9141 172.023S49.038 153.025 44.494 157.976C40.304 162.539 46.765 166.418 56.7211 172.397 57.5671 172.905 58.4391 173.429 59.3321 173.969 70.7231 180.865 71.609 182.684 69.992 185.293 69.395 186.257 65.582 183.968 60.892 181.153 52.897 176.352 42.3551 170.023 40.8661 175.688 39.5781 180.591 47.334 183.595 54.368 186.32 60.228 188.59 65.5881 190.666 64.7991 193.484 63.9821 196.406 59.5531 193.969 54.7121 191.305 49.2771 188.314 43.3221 185.038 41.3731 188.735 37.6901 195.725 66.7831 203.954 67.0231 204.015 76.4231 206.453 100.295 211.619 108.638 199.391zm38.665.0C141.301 190.59 141.727 183.984 149.962 175.754 158.197 167.523 162.99 155.487 162.99 155.487S164.779 148.496 168.859 149.138C172.939 149.78 175.932 160.227 167.39 166.622 158.847 173.017 169.089 177.353 172.377 171.352 175.666 165.35 184.637 149.923 189.291 146.972 193.945 144.021 197.22 145.675 196.122 151.757 195.578 154.776 190.509 159.929 185.774 164.744 180.97 169.628 176.509 174.163 177.462 175.768c1.893 3.18700000000001 8.565-3.749 8.565-3.749s20.875-18.997 25.421-14.046C215.637 162.535 209.176 166.415 199.219 172.394 198.348 172.917 197.478 173.441 196.609 173.966 185.218 180.862 184.332 182.681 185.948 185.289 186.546 186.254 190.359 183.964 195.048 181.149 203.044 176.349 213.586 170.019 215.075 175.685 216.364 180.588 208.607 183.592 201.573 186.317 195.713 188.587 190.353 190.663 191.141 193.481 191.957 196.402 196.385 193.965 201.225 191.301 206.66 188.31 212.616 185.032 214.564 188.732c3.684 6.994-25.414 15.215-25.649 15.275C179.515 206.453 155.643 211.619 147.303 199.391z" fill="#ffd21e"/><path fill-rule="evenodd" clip-rule="evenodd" d="M152.047 102.567C153.229 102.985 154.108 104.257 154.944 105.468 156.074 107.104 157.126 108.627 158.74 107.769 160.644 106.756 162.205 105.202 163.225 103.302 164.246 101.402 164.681 99.2427 164.475 97.096 164.321 95.4908 163.813 93.9398 162.987 92.5548 162.161 91.1697 161.038 89.985 159.7 89.0862 158.361 88.1874 156.839 87.5968 155.245 87.3569 153.65 87.117 152.022 87.2339 150.478 87.699 148.934 88.1639 147.513 88.9653 146.316 90.0455 145.119 91.1257 144.176 92.4578 143.556 93.946 142.936 95.4342 142.653 97.0415 142.728 98.652 142.804 100.263 143.235 101.836 143.992 103.26 144.74 104.667 146.4 104.003 148.152 103.302 149.525 102.753 150.956 102.181 152.047 102.567zm-51.375.0C99.49 102.985 98.611 104.258 97.775 105.468 96.645 107.105 95.592 108.627 93.979 107.769 91.5845 106.501 89.7482 104.386 88.8278 101.838 87.9075 99.2895 87.9692 96.4896 89.0008 93.9841S91.9601 89.4471 94.408 88.2855C96.856 87.1239 99.6488 86.9156 102.242 87.701 104.307 88.3228 106.141 89.5427 107.513 91.2065 108.885 92.8704 109.732 94.9035 109.949 97.049 110.165 99.1945 109.74 101.356 108.728 103.26 107.979 104.667 106.319 104.003 104.567 103.303 103.193 102.753 101.764 102.181 100.672 102.567zm43.427 46.751C152.242 142.903 155.233 132.429 155.233 125.977 155.233 120.877 151.802 122.482 146.309 125.202L145.999 125.355C140.957 127.852 134.245 131.177 126.877 131.177 119.508 131.177 112.796 127.852 107.755 125.354 102.084 122.545 98.527 120.783 98.527 125.978c0 6.65599999999999 3.182 17.585 11.916 23.934C111.596 147.573 113.219 145.497 115.211 143.813 117.202 142.129 119.52 140.874 122.018 140.126 122.89 139.866 123.788 141.367 124.707 142.904 125.594 144.386 126.501 145.902 127.423 145.902 128.406 145.902 129.371 144.408 130.314 142.95 131.299 141.425 132.26 139.94 133.189 140.237 137.864 141.738 141.775 144.993 144.099 149.318z" fill="#32343d"/><path d="M144.097 149.317C139.856 152.659 134.219 154.9 126.878 154.9 119.981 154.9 114.587 152.922 110.443 149.911 111.596 147.572 113.219 145.495 115.211 143.812 117.202 142.128 119.52 140.873 122.018 140.125 123.73 139.614 125.545 145.901 127.423 145.901 129.433 145.901 131.37 139.655 133.189 140.236 137.863 141.738 141.773 144.993 144.097 149.317z" fill="#ff323d"/><path fill-rule="evenodd" clip-rule="evenodd" d="M81.2 111.64C80.2312 112.288 79.1173 112.687 77.9572 112.801 76.7971 112.916 75.6267 112.742 74.55 112.295 73.6893 111.94 72.9072 111.418 72.2488 110.759 71.5903 110.101 71.0684 109.319 70.713 108.458 70.267 107.381 70.0935 106.211 70.2082 105.051 70.3228 103.891 70.7219 102.777 71.37 101.808 72.1488 100.642 73.2558 99.7333 74.5512 99.1967 75.8466 98.6601 77.272 98.5197 78.6471 98.7935 80.0223 99.0672 81.2853 99.7427 82.2764 100.734 83.2675 101.726 83.9422 102.99 84.215 104.365 84.4883 105.74 84.3477 107.165 83.8113 108.46 83.2748 109.755 82.3654 110.861 81.2 111.64zm101.413.0C181.644 112.288 180.53 112.687 179.37 112.801 178.209 112.916 177.039 112.742 175.962 112.295 175.101 111.939 174.319 111.418 173.661 110.759 173.003 110.101 172.481 109.319 172.125 108.458 171.68 107.381 171.507 106.211 171.621 105.051 171.736 103.891 172.135 102.777 172.782 101.808 173.364 100.936 174.133 100.205 175.032 99.6658 175.931 99.1269 176.938 98.7942 177.981 98.6917 179.025 98.5891 180.078 98.7193 181.064 99.0728 182.051 99.4264 182.947 99.9944 183.688 100.736 184.68 101.727 185.355 102.99 185.628 104.365 185.902 105.74 185.761 107.165 185.224 108.46 184.687 109.755 183.779 110.861 182.613 111.64z" fill="#ffad03"/></svg>
</span></span><span>2024-11-01</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2410.20650 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2410.20650 target=_self role=button>‚Üó Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/neuzip-memory-efficient-training-and target=_self role=button>‚Üó Papers with Code</a></p><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Training and deploying large neural networks is hampered by <strong>limited on-device memory</strong>. While techniques like quantization exist, they often compromise model performance. This paper introduces a novel solution to this problem.</p><p>The proposed method, NeuZip, uses a <strong>lossless compression algorithm</strong> for training, focusing on the low-entropy nature of the exponent bits in floating-point numbers. For inference, a lossy variant offers further memory reduction by controlling the relative change of each parameter. Experiments on various models showed that NeuZip significantly reduces memory usage (e.g., Llama-3 8B model training memory reduced from 31GB to under 16GB) while maintaining, or even improving, performance, surpassing existing techniques like quantization.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-0fd59b311c2ca4ffe2329a0e8de4efdb></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-0fd59b311c2ca4ffe2329a0e8de4efdb",{strings:[" NeuZip significantly reduces the memory usage during both training and inference of large neural networks without sacrificing performance. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-ea8861b40a9687850bb852fe25ed7a35></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-ea8861b40a9687850bb852fe25ed7a35",{strings:[" NeuZip employs a novel weight compression scheme based on the entropy of floating-point numbers, achieving high compression ratios. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-3b6dc47b4523be656e36c977bc401c40></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-3b6dc47b4523be656e36c977bc401c40",{strings:[" Lossy NeuZip offers a flexible trade-off between memory usage and performance, particularly beneficial for inference tasks. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is important because it presents <strong>NeuZip</strong>, a novel and effective method for memory-efficient training and inference of large neural networks. This addresses a critical limitation in deep learning, enabling researchers to train and deploy larger, more powerful models with limited resources. The proposed technique offers a <strong>significant improvement over existing methods</strong>, opening up new avenues for research in memory optimization and large model deployment.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2410.20650/x1.png alt></figure></p><blockquote><p>üîº Figure 1 presents histograms visualizing the distribution of sign bits, exponent bits, and mantissa bits within the parameters of the LLama-3 8B model. Each histogram shows the frequency of occurrence for each possible binary value (represented on the x-axis), providing insights into the entropy of each component. This analysis is crucial to understanding NeuZip&rsquo;s compression strategy, which focuses on the low-entropy nature of specific bits to achieve memory efficiency.</p><details><summary>read the caption</summary>Figure 1: The histograms of different components of the parameters of LLama-3 8B model¬†(Dubey et¬†al., 2024). The xùë•xitalic_x-axis is all possible binary values and the yùë¶yitalic_y-axis represent the frequency of each value.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Name</th><th>GPT-Neo-XL 2.7B Loss</th><th>GPT-Neo-XL 2.7B Mem</th><th>GPT-Neo-XL 2.7B Speed</th><th>Llama-3 8B Loss</th><th>Llama-3 8B Mem</th><th>Llama-3 8B Speed</th><th>LLama-2 13B Loss</th><th>LLama-2 13B Mem</th><th>LLama-2 13B Speed</th></tr></thead><tbody><tr><td>Vanilla</td><td><strong>8.81</strong></td><td>11.22</td><td><strong>0.96</strong></td><td><strong>8.61</strong></td><td>30.97</td><td>0.77</td><td>-</td><td>OOM</td><td>-</td></tr><tr><td>LOMO</td><td><strong>8.81</strong></td><td>6.97</td><td>0.94</td><td><strong>8.61</strong></td><td>19.47</td><td><strong>0.78</strong></td><td><strong>9.10</strong></td><td>26.26</td><td><strong>0.49</strong></td></tr><tr><td>+NeuZip Lossless</td><td><strong>8.81</strong></td><td><strong>5.54</strong></td><td>0.70</td><td><strong>8.61</strong></td><td><strong>15.25</strong></td><td>0.45</td><td><strong>9.10</strong></td><td>18.58</td><td>0.28</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of pre-training three different decoder-only language models (GPT-Neo-XL 2.7B, Llama-3 8B, and Llama-2 13B) on a language modeling task. The models were trained using four different methods: a standard training approach (Vanilla), an approach using the Layer-wise Optimization with Memory Optimization (LOMO) technique, LOMO combined with lossless NeuZip compression, and LOMO combined with lossless NeuZip. The table shows for each model and training method the cross-entropy loss calculated on a validation set, the peak memory usage during training (measured in gibibytes, GiB), and the training speed (number of iterations per second). The best performing method for each model is highlighted in bold.</p><details><summary>read the caption</summary>Table 1: Pre-training decoder-only models on the language modeling task. The loss numbers are calculated on the validation set with the cross-entropy loss. Memory is reported in GiB (10243superscript102431024^{3}1024 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT B). Speed represents the number of iterations per second. The bold numbers represent the top results.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Low-Entropy Weights<div id=low-entropy-weights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#low-entropy-weights aria-label=Anchor>#</a></span></h4><p>The research paper section on &ldquo;Low-Entropy Nature of Neural Network Parameters&rdquo; posits that neural network weights exhibit low entropy. This is primarily attributed to <strong>weight initialization strategies</strong>, which often center weights around zero (e.g., Gaussian initialization), and the effects of <strong>regularization techniques</strong> (e.g., weight decay) that consistently reduce weight magnitudes during training. This central tendency, alongside the implicit regularization effect of stochastic gradient descent, contributes to the low-entropy characteristic. The paper highlights that this property, specifically the low entropy of exponent bits in the floating-point representation of weights, makes these weights highly compressible, offering a pathway for efficient memory management in training and inference without significantly sacrificing model performance. The <strong>low entropy is key</strong> to the success of NeuZip&rsquo;s compression algorithm, as it forms the fundamental basis for achieving significant memory savings.</p><h4 class="relative group">ANS Compression<div id=ans-compression class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#ans-compression aria-label=Anchor>#</a></span></h4><p>The research paper introduces Asymmetric Numeral Systems (ANS) as a <strong>lossless compression algorithm</strong> for the exponent bits of floating-point numbers in neural network weights. This is motivated by the observation that the exponent bits exhibit low entropy due to the concentration of weights around zero, a common characteristic resulting from initialization and training dynamics. ANS is chosen due to its <strong>high throughput on parallel computing devices</strong> like GPUs, essential for efficient training. <strong>Lossless compression ensures that no precision is lost during training</strong>, maintaining the full capability of the network while simultaneously reducing memory usage. The algorithm efficiently handles the dynamic range of exponents by treating each bit as a base-n number with a variable base determined by symbol frequency. The authors leverage the multi-layer structure of neural networks to compress and decompress on a per-layer basis, minimizing the overall memory footprint, and fully preserving backpropagation capabilities. The choice of ANS allows NeuZip to successfully reduce memory consumption without sacrificing model performance during training.</p><h4 class="relative group">Lossy Inference<div id=lossy-inference class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#lossy-inference aria-label=Anchor>#</a></span></h4><p>The research paper introduces NeuZip, a memory-efficient technique for neural network training and inference. Its lossy inference component focuses on <strong>reducing memory usage during inference</strong> by selectively compressing the mantissa bits of floating-point numbers. This is motivated by the observation that inference is less sensitive to precision loss than training. By controlling the relative change in each parameter through <strong>controlled rounding and truncation of mantissa bits</strong>, NeuZip achieves significant memory reduction. The effectiveness is empirically demonstrated on various models and tasks, showing a favorable trade-off between memory usage and performance. <strong>Lossy NeuZip is presented as a practical approach to enable deployment of large models on resource-constrained devices</strong>, maintaining high accuracy despite the lossy compression scheme.</p><h4 class="relative group">Memory Benchmarks<div id=memory-benchmarks class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#memory-benchmarks aria-label=Anchor>#</a></span></h4><p>The provided text does not contain a heading explicitly titled &lsquo;Memory Benchmarks&rsquo;. Therefore, a summary cannot be generated. To create the summary, please provide the relevant text from the PDF&rsquo;s section on memory benchmarks. The summary would then analyze the memory usage results reported for various models and techniques (e.g., vanilla training, LOMO, NeuZip variants, and quantization methods) to determine their memory efficiency. It would likely highlight the <strong>significant memory savings achieved by the proposed NeuZip method, especially compared to the baseline and quantization approaches</strong>, while maintaining or even improving model performance. The summary might also touch upon the impact of hyperparameter choices such as block size on memory usage and performance trade-offs, focusing on <strong>NeuZip&rsquo;s position on the Pareto frontier</strong>, which indicates a superior memory-performance balance. In addition, the summary might discuss the memory efficiency achieved during both training and inference phases and emphasize the <strong>achievability of training large language models (LLMs) on consumer-grade GPUs due to NeuZip&rsquo;s efficiency</strong>.</p><h4 class="relative group">Future Directions<div id=future-directions class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-directions aria-label=Anchor>#</a></span></h4><p>The research paper does not include a section specifically titled &lsquo;Future Directions&rsquo;. Therefore, it is not possible to provide a summary about a heading that does not exist in the provided document. To generate the requested summary, please provide a PDF containing a &lsquo;Future Directions&rsquo; section.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2410.20650/x2.png alt></figure></p><blockquote><p>üîº This figure illustrates the reverse-mode automatic differentiation (backpropagation) process for a linear layer in a neural network, comparing different memory-saving techniques. (a) Vanilla shows the standard approach, where both weights and activations/gradients are stored in memory throughout the entire process. This contrasts with methods like (b) activation checkpointing (AC), (c) AC combined with Low-Memory Optimization (LOMO), and (d) NeuZip. These optimized techniques utilize various strategies to reduce memory usage during backpropagation, either by recomputing certain values or leveraging compressed representations, as seen in NeuZip&rsquo;s compressed weight storage.</p><details><summary>read the caption</summary>(a) Vanilla</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2410.20650/x3.png alt></figure></p><blockquote><p>üîº This figure shows the memory usage pattern of the activation checkpointing (AC) method for a linear layer in a neural network during reverse-mode automatic differentiation (backpropagation). Blue blocks represent data temporarily loaded into memory for calculations, while red blocks denote data constantly residing in memory. Activation checkpointing saves memory by recomputing activations during backpropagation, but still needs to store weights and other intermediate variables. The image visually compares vanilla, AC, AC with Layer-wise Optimization using Memory Optimization (LOMO), and NeuZip, which is the proposed method in the paper.</p><details><summary>read the caption</summary>(b) AC</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2410.20650/x4.png alt></figure></p><blockquote><p>üîº This figure shows the reverse-mode automatic differentiation (backpropagation) process for a linear layer in a neural network using the AC+LOMO memory-saving technique. Blue blocks represent data temporarily loaded into memory during computation for the current layer, while red blocks show data that remains in memory throughout the entire training process. AC+LOMO combines activation checkpointing (AC) and Layer-wise Ordering of Memory Optimization (LOMO) to reduce memory usage. Activation checkpointing recomputes activations instead of storing them, while LOMO optimizes memory usage by efficiently managing memory allocation and deallocation across layers. This visualization contrasts AC+LOMO with other memory-saving approaches, highlighting its efficiency in reducing peak memory usage during training.</p><details><summary>read the caption</summary>(c) AC+LOMO</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2410.20650/x5.png alt></figure></p><blockquote><p>üîº This figure shows a diagram illustrating the reverse-mode automatic differentiation (backpropagation) process in a linear layer of a neural network using NeuZip. It compares NeuZip&rsquo;s memory-saving approach with other methods like vanilla, activation checkpointing (AC), and AC combined with LOMO. Blue blocks represent data temporarily loaded into memory, while red blocks represent data persistently stored in memory. NeuZip significantly reduces memory usage by compressing weight matrices and utilizing the multi-layer structure of neural networks to avoid storing large buffers.</p><details><summary>read the caption</summary>(d) NeuZip</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2410.20650/x6.png alt></figure></p><blockquote><p>üîº This figure illustrates the memory usage of different training methods for a single linear layer during backpropagation. It compares vanilla training, activation checkpointing (AC), AC with Layer-wise Optimization using Memory Optimization (AC+LOMO), and the proposed NeuZip method. Blue blocks represent data loaded into memory temporarily for a single layer&rsquo;s computation, while red blocks denote data persistently stored throughout training. NeuZip is shown to reduce memory usage by strategically compressing parameters.</p><details><summary>read the caption</summary>Figure 2: Reverse-mode automatic differentiation (e.g., back-propagation) with different memory-saving techniques for a linear layer. Blocks colored blue are loaded in memory temporarily for the calculation of this layer, whereas the blocks colored red are always in memory throughout training.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2410.20650/x7.png alt></figure></p><blockquote><p>üîº This figure illustrates the Pareto frontier for different model compression techniques, showing the trade-off between memory usage and model performance. The x-axis represents memory consumption (in GiB), and the y-axis represents model performance (e.g., perplexity). Three different model sizes (Llama-3 8B, Llama-2 13B, Yi-1.5 34B) are shown, each with results for a vanilla (uncompressed) model, a quantization method, and several NeuZip variants. Points closer to the bottom-left corner indicate better memory efficiency and higher performance. The results demonstrate that NeuZip variants generally lie closer to or on the Pareto frontier compared to quantization methods, indicating a better balance between memory efficiency and performance.</p><details><summary>read the caption</summary>Figure 3: The trade-off between memory and performance for different methods.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2410.20650/x8.png alt></figure></p><blockquote><p>üîº This figure compares the throughput (in GiB/s) of various methods for compressing and decompressing matrices of varying sizes in neural network training. Panel (a) shows the compression throughput of CPU offloading, quantization, lossy NeuZip, and lossless NeuZip. Panel (b) displays the decompression throughput of GPU reloading, de-quantization, lossy NeuZip decompression, and lossless NeuZip decompression. The results illustrate the relative efficiency of each method in terms of data transfer rate and memory usage.</p><details><summary>read the caption</summary>Figure 4: The throughput experiment. (a) Comparison of CPU-offloading, quantization, lossy NeuZip compression, and lossless NeuZip compression. (b) Comparison of GPU-reloading, de-quantization, lossy NeuZip decompression, and lossless NeuZip decompression.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2410.20650/x9.png alt></figure></p><blockquote><p>üîº This figure displays histograms illustrating the distribution of sign bits, exponent bits, and mantissa bits within the floating-point numbers representing the parameters of a randomly initialized Llama-3 8B model. The x-axis of each histogram represents the possible values for each component (bits), while the y-axis represents the frequency of occurrence for each value in the model&rsquo;s parameters. The histograms visually demonstrate the low entropy nature of the exponent bits, a key observation supporting the NeuZip compression method described in the paper.</p><details><summary>read the caption</summary>Figure 5: The histograms of different floating-point components of the parameters of a randomly initialized Llama-3 8B model.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Name</th><th>T5 1B BLEU</th><th>T5 1B Mem</th><th>T5 1B Speed</th><th>T5 3B BLEU</th><th>T5 3B Mem</th><th>T5 3B Speed</th><th>T5 11B BLEU</th><th>T5 11B Mem</th><th>T5 11B Speed</th></tr></thead><tbody><tr><td>Vanilla</td><td>79.9</td><td>3.82</td><td>3.69</td><td>85.1</td><td>11.32</td><td>2.43</td><td>-</td><td>OOM</td><td>-</td></tr><tr><td>LOMO</td><td>79.9</td><td>2.75</td><td>3.68</td><td>85.1</td><td>7.07</td><td>2.47</td><td>82.3</td><td>25.95</td><td>0.69</td></tr><tr><td>+ NeuZip Lossless</td><td>79.9</td><td>2.39</td><td>2.02</td><td>85.1</td><td>5.21</td><td>1.33</td><td>82.3</td><td>20.68</td><td>0.46</td></tr><tr><td>QLoRA INT8</td><td>70.4</td><td>5.84</td><td>1.11</td><td>72.1</td><td>11.54</td><td>1.12</td><td>63.5</td><td>33.36</td><td>0.37</td></tr><tr><td>QLoRA FP4</td><td>70.1</td><td>3.63</td><td>1.70</td><td>72.1</td><td>7.35</td><td>1.74</td><td>63.3</td><td>22.73</td><td>0.58</td></tr><tr><td>QLoRA FP4<sup>2</sup></td><td>70.6</td><td>3.61</td><td>1.63</td><td>72.0</td><td>7.27</td><td>1.61</td><td>60.6</td><td>22.38</td><td>0.57</td></tr><tr><td>QLoRA NF4</td><td>70.4</td><td>3.63</td><td>1.83</td><td>71.2</td><td>7.35</td><td>1.65</td><td>59.4</td><td>22.73</td><td>0.57</td></tr><tr><td>QLoRA NF4<sup>2</sup></td><td>70.5</td><td>3.61</td><td>1.64</td><td>71.2</td><td>7.07</td><td>1.57</td><td>57.9</td><td>22.38</td><td>0.57</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of fine-tuning various encoder-decoder models on an SQL generation task. It compares different model compression techniques (including the proposed NeuZip method) in terms of their impact on model performance (measured by BLEU score using SacreBLEU), memory usage (reported in GiB), and training speed (iterations per second). The top-performing model for each metric in each model size is highlighted in bold.</p><details><summary>read the caption</summary>Table 2: Fine-tuning encoder‚Äìdecoder models on the SQL generation task. The BLEU scores are calculated with SacreBLEU. Memory is reported in GiB (10243superscript102431024^{3}1024 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT B). Speed represents the number of iterations per second. The bold numbers represent the top results.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Name</th><th>Llama-3 8B PPL</th><th>Llama-3 8B Mem</th><th>Llama-3 8B Speed</th><th>Llama-2 13B PPL</th><th>Llama-2 13B Mem</th><th>Llama-2 13B Speed</th><th>Yi-1.5 34B PPL</th><th>Yi-1.5 34B Mem</th><th>Yi-1.5 34B Speed</th></tr></thead><tbody><tr><td>Vanilla</td><td>9.89</td><td>15.08</td><td>5.07</td><td>10.87</td><td>24.36</td><td>3.59</td><td>-</td><td>OOM</td><td>-</td></tr><tr><td>Quant INT8</td><td>10.07</td><td>8.63</td><td>3.54</td><td>10.97</td><td>12.74</td><td>2.27</td><td>10.87</td><td>33.41</td><td>1.13</td></tr><tr><td>Quant FP4</td><td>11.51</td><td>5.77</td><td>3.45</td><td>11.38</td><td>7.37</td><td>1.87</td><td>11.57</td><td>19.54</td><td>1.75</td></tr><tr><td>Quant NF4</td><td>10.75</td><td>5.77</td><td>3.38</td><td>11.15</td><td>7.37</td><td>1.83</td><td>11.06</td><td>19.54</td><td>1.67</td></tr><tr><td>Quant FP4<sup>2</sup></td><td>11.50</td><td>5.44</td><td>3.41</td><td>11.38</td><td>6.87</td><td>1.86</td><td>11.57</td><td>18.11</td><td>1.61</td></tr><tr><td>Quant NF4<sup>2</sup></td><td>10.75</td><td>5.44</td><td>3.34</td><td>11.15</td><td>6.87</td><td>1.81</td><td>11.06</td><td>18.11</td><td>1.54</td></tr><tr><td>NeuZip 0-bit</td><td>13.64</td><td>5.24</td><td>3.44</td><td>12.46</td><td>6.30</td><td>1.87</td><td>12.06</td><td>16.20</td><td>0.94</td></tr><tr><td>NeuZip 1-bit</td><td>10.77</td><td>6.05</td><td>3.38</td><td>11.17</td><td>7.77</td><td>1.86</td><td>11.04</td><td>20.14</td><td>0.93</td></tr><tr><td>NeuZip 3-bit</td><td>9.93</td><td>7.70</td><td>3.38</td><td>10.90</td><td>10.73</td><td>1.84</td><td>10.76</td><td>27.92</td><td>0.93</td></tr><tr><td>NeuZip 7-bit (lossless)</td><td>9.89</td><td>10.95</td><td>3.39</td><td>10.87</td><td>16.66</td><td>1.84</td><td>10.72</td><td>43.40</td><td>0.94</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 3 presents a comprehensive evaluation of the lossy NeuZip compression technique on various neural network models and tasks. It compares the performance (perplexity), memory usage (in GiB), and training speed (iterations per second) of lossy NeuZip against several baseline methods, including standard models and various quantization techniques (INT8, FP4, NF4). The table shows the perplexity scores, memory requirements, and iteration speeds achieved by each method, enabling a detailed comparison of the trade-off between memory efficiency and model accuracy. The bold values indicate the best results for each model and task, while the underlined numbers highlight the second-best performing methods.</p><details><summary>read the caption</summary>Table 3: Evaluating lossy NeuZip on different models and tasks. ‚ÄòPPL‚Äù represents the perplexity values. Memory is reported in GiB. Speed represents the number of iterations per second. The bold numbers represent the top results, whereas the underlined numbers are the second-best ones.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Name</th><th>T5 1B PPL</th><th>T5 1B Mem</th><th>T5 1B Speed</th><th>T5 3B PPL</th><th>T5 3B Mem</th><th>T5 3B Speed</th><th>T5 11B PPL</th><th>T5 11B Mem</th><th>T5 11B Speed</th></tr></thead><tbody><tr><td>Vanilla</td><td><strong>2.614</strong></td><td>1.37</td><td><strong>23.73</strong></td><td><strong>2.571</strong></td><td>5.31</td><td><strong>19.86</strong></td><td><strong>2.568</strong></td><td>21.06</td><td><strong>6.20</strong></td></tr><tr><td>Quant INT8</td><td>2.615</td><td>1.28</td><td>4.24</td><td><strong>2.573</strong></td><td>4.94</td><td>4.28</td><td><strong>2.569</strong></td><td>19.59</td><td>2.58</td></tr><tr><td>Quant NF4</td><td>2.632</td><td>1.08</td><td>11.64</td><td>2.588</td><td>4.12</td><td>11.82</td><td>2.579</td><td>16.28</td><td>4.48</td></tr><tr><td>Quant FP4</td><td>2.646</td><td>1.08</td><td>11.92</td><td>2.594</td><td>4.12</td><td><strong>11.99</strong></td><td>2.585</td><td>16.28</td><td><strong>4.59</strong></td></tr><tr><td>Quant FP4<sup>2</sup></td><td>2.646</td><td>1.05</td><td>10.39</td><td>2.594</td><td>4.03</td><td>9.72</td><td>2.585</td><td>15.93</td><td>4.52</td></tr><tr><td>Quant NF4<sup>2</sup></td><td>2.632</td><td>1.05</td><td>10.39</td><td>2.587</td><td>4.03</td><td>9.96</td><td>2.579</td><td>15.93</td><td>4.39</td></tr><tr><td>NeuZip 0-bit</td><td>2.731</td><td><strong>0.40</strong></td><td>11.82</td><td>2.668</td><td><strong>1.41</strong></td><td>8.70</td><td>2.651</td><td><strong>5.35</strong></td><td>3.24</td></tr><tr><td>NeuZip 1-bit</td><td>2.641</td><td><strong>0.48</strong></td><td>11.68</td><td>2.591</td><td><strong>1.78</strong></td><td>8.61</td><td>2.581</td><td><strong>6.65</strong></td><td>3.21</td></tr><tr><td>NeuZip 3-bit</td><td><strong>2.614</strong></td><td>0.66</td><td><strong>11.99</strong></td><td>2.574</td><td>2.42</td><td>8.60</td><td><strong>2.569</strong></td><td>9.27</td><td>3.19</td></tr><tr><td>NeuZip 7-bit (lossless)</td><td><strong>2.614</strong></td><td>0.99</td><td>11.55</td><td><strong>2.571</strong></td><td>3.73</td><td>8.77</td><td><strong>2.568</strong></td><td>14.46</td><td>3.23</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of evaluating decoder-only language models on a language modeling task. The models are compared across three metrics: perplexity (PPL), memory usage (in GiB), and training speed (iterations per second). Perplexity scores are adjusted to the word level to allow for fair comparison across models with different tokenization schemes. The table includes results for a vanilla (uncompressed) model, several quantization methods (INT8, FP4, NF4, FP42, NF42), and different variations of the NeuZip algorithm (0-bit, 1-bit, 3-bit, and 7-bit (lossless)). This allows for a comprehensive comparison of model performance, efficiency, and memory footprint.</p><details><summary>read the caption</summary>(a) Evaluating decoder-only models on the language modeling task. Here, the perplexities are adjusted to word level to compare across different tokenizations.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Name</th><th>Block 32</th><th>Block 32</th><th>Block 64</th><th>Block 64</th><th>Block 128</th><th>Block 128</th><th>Block 256</th><th>Block 256</th><th>Block 512</th><th>Block 512</th></tr></thead><tbody><tr><td></td><td>PPL</td><td>Mem</td><td>PPL</td><td>Mem</td><td>PPL</td><td>Mem</td><td>PPL</td><td>Mem</td><td>PPL</td><td>Mem</td></tr><tr><td>NeuZip 0-bit</td><td>6.341</td><td>35.7</td><td>6.694</td><td>34.6</td><td>6.853</td><td>34.2</td><td>7.639</td><td>33.8</td><td>7.104</td><td>33.5</td></tr><tr><td>NeuZip 1-bit</td><td>-</td><td>OOM</td><td>4.611</td><td>42.7</td><td>4.662</td><td>42.2</td><td>4.640</td><td>41.8</td><td>4.649</td><td>41.4</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of evaluating encoder-decoder models (T5 models of various sizes) on a language modeling task. Because all models used the same tokenizer, perplexity is reported at the token level for simpler comparison and easier interpretation of the results. The table likely shows metrics like perplexity (PPL), memory usage (Mem), and training speed (Speed) for different model sizes and/or compression techniques. The focus is on comparing the impact of different methods on efficiency and accuracy.</p><details><summary>read the caption</summary>(b) Evaluating encoder‚Äìdecoder models on the language modeling task. Since all models use the same tokenizer, we reported perplexities at the token level for simplicity.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-1b371fce9037f4edf070262a541c2117 class=gallery><img src=https://ai-paper-reviewer.com/2410.20650/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2410.20650/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2410.20650/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2410.20650/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2410.20650/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2410.20650/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2410.20650/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2410.20650/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2410.20650/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2410.20650/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2410.20650/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2410.20650/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2410.20650/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2410.20650/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2410.20650/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2410.20650/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2410.20650/17.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/&amp;title=NeuZip:%20Memory-Efficient%20Training%20and%20Inference%20with%20Dynamic%20Compression%20of%20Neural%20Networks" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/&amp;text=NeuZip:%20Memory-Efficient%20Training%20and%20Inference%20with%20Dynamic%20Compression%20of%20Neural%20Networks" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/&amp;subject=NeuZip:%20Memory-Efficient%20Training%20and%20Inference%20with%20Dynamic%20Compression%20of%20Neural%20Networks" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2410.20650/index.md",oid_likes="likes_paper-reviews/2410.20650/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2410.22370/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Survey of User Interface Design and Interaction Techniques in Generative AI Applications</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-10-28T00:00:00+00:00>28 October 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2410.21157/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-10-28T00:00:00+00:00>28 October 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
AI Paper Reviews by AI</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>