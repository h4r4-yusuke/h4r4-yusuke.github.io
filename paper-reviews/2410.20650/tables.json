[{"content": "| Name | GPT-Neo-XL 2.7B Loss | GPT-Neo-XL 2.7B Mem | GPT-Neo-XL 2.7B Speed | Llama-3 8B Loss | Llama-3 8B Mem | Llama-3 8B Speed | LLama-2 13B Loss | LLama-2 13B Mem | LLama-2 13B Speed |\n|---|---|---|---|---|---|---|---|---|---| \n| Vanilla | **8.81** | 11.22 | **0.96** | **8.61** | 30.97 | 0.77 | - | OOM | - |\n| LOMO | **8.81** | 6.97 | 0.94 | **8.61** | 19.47 | **0.78** | **9.10** | 26.26 | **0.49** |\n| +NeuZip Lossless | **8.81** | **5.54** | 0.70 | **8.61** | **15.25** | 0.45 | **9.10** | 18.58 | 0.28 |", "caption": "Table 1: Pre-training decoder-only models on the language modeling task. The loss numbers are calculated on the validation set with the cross-entropy loss. Memory is reported in GiB (10243superscript102431024^{3}1024 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT B). Speed represents the number of iterations per second. The bold numbers represent the top results.", "description": "This table presents the results of pre-training three different decoder-only language models (GPT-Neo-XL 2.7B, Llama-3 8B, and Llama-2 13B) on a language modeling task.  The models were trained using four different methods: a standard training approach (Vanilla), an approach using the Layer-wise Optimization with Memory Optimization (LOMO) technique, LOMO combined with lossless NeuZip compression, and LOMO combined with lossless NeuZip.  The table shows for each model and training method the cross-entropy loss calculated on a validation set, the peak memory usage during training (measured in gibibytes, GiB), and the training speed (number of iterations per second).  The best performing method for each model is highlighted in bold.", "section": "3.1 Lossless NeuZip for Pre-Training"}, {"content": "| Name | T5 1B BLEU | T5 1B Mem | T5 1B Speed | T5 3B BLEU | T5 3B Mem | T5 3B Speed | T5 11B BLEU | T5 11B Mem | T5 11B Speed |\n|---|---|---|---|---|---|---|---|---|---| \n| Vanilla | 79.9 | 3.82 | 3.69 | 85.1 | 11.32 | 2.43 | - | OOM | - |\n| LOMO | 79.9 | 2.75 | 3.68 | 85.1 | 7.07 | 2.47 | 82.3 | 25.95 | 0.69 |\n| + NeuZip Lossless | 79.9 | 2.39 | 2.02 | 85.1 | 5.21 | 1.33 | 82.3 | 20.68 | 0.46 |\n| QLoRA INT8 | 70.4 | 5.84 | 1.11 | 72.1 | 11.54 | 1.12 | 63.5 | 33.36 | 0.37 |\n| QLoRA FP4 | 70.1 | 3.63 | 1.70 | 72.1 | 7.35 | 1.74 | 63.3 | 22.73 | 0.58 |\n| QLoRA FP4<sup>2</sup> | 70.6 | 3.61 | 1.63 | 72.0 | 7.27 | 1.61 | 60.6 | 22.38 | 0.57 |\n| QLoRA NF4 | 70.4 | 3.63 | 1.83 | 71.2 | 7.35 | 1.65 | 59.4 | 22.73 | 0.57 |\n| QLoRA NF4<sup>2</sup> | 70.5 | 3.61 | 1.64 | 71.2 | 7.07 | 1.57 | 57.9 | 22.38 | 0.57 |", "caption": "Table 2: Fine-tuning encoder\u2013decoder models on the SQL generation task. The BLEU scores are calculated with SacreBLEU. Memory is reported in GiB (10243superscript102431024^{3}1024 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT B). Speed represents the number of iterations per second. The bold numbers represent the top results.", "description": "This table presents the results of fine-tuning various encoder-decoder models on an SQL generation task.  It compares different model compression techniques (including the proposed NeuZip method) in terms of their impact on model performance (measured by BLEU score using SacreBLEU), memory usage (reported in GiB), and training speed (iterations per second).  The top-performing model for each metric in each model size is highlighted in bold.", "section": "3.2 Lossless NeuZip for Fine-Tuning"}, {"content": "| Name | Llama-3 8B PPL | Llama-3 8B Mem | Llama-3 8B Speed | Llama-2 13B PPL | Llama-2 13B Mem | Llama-2 13B Speed | Yi-1.5 34B PPL | Yi-1.5 34B Mem | Yi-1.5 34B Speed |\n|---|---|---|---|---|---|---|---|---|---|\n| Vanilla | 9.89 | 15.08 | 5.07 | 10.87 | 24.36 | 3.59 | - | OOM | - |\n| Quant INT8 | 10.07 | 8.63 | 3.54 | 10.97 | 12.74 | 2.27 | 10.87 | 33.41 | 1.13 |\n| Quant FP4 | 11.51 | 5.77 | 3.45 | 11.38 | 7.37 | 1.87 | 11.57 | 19.54 | 1.75 |\n| Quant NF4 | 10.75 | 5.77 | 3.38 | 11.15 | 7.37 | 1.83 | 11.06 | 19.54 | 1.67 |\n| Quant FP4<sup>2</sup> | 11.50 | 5.44 | 3.41 | 11.38 | 6.87 | 1.86 | 11.57 | 18.11 | 1.61 |\n| Quant NF4<sup>2</sup> | 10.75 | 5.44 | 3.34 | 11.15 | 6.87 | 1.81 | 11.06 | 18.11 | 1.54 |\n| NeuZip 0-bit | 13.64 | 5.24 | 3.44 | 12.46 | 6.30 | 1.87 | 12.06 | 16.20 | 0.94 |\n| NeuZip 1-bit | 10.77 | 6.05 | 3.38 | 11.17 | 7.77 | 1.86 | 11.04 | 20.14 | 0.93 |\n| NeuZip 3-bit | 9.93 | 7.70 | 3.38 | 10.90 | 10.73 | 1.84 | 10.76 | 27.92 | 0.93 |\n| NeuZip 7-bit (lossless) | 9.89 | 10.95 | 3.39 | 10.87 | 16.66 | 1.84 | 10.72 | 43.40 | 0.94 |", "caption": "Table 3: Evaluating lossy NeuZip on different models and tasks. \u2018PPL\u201d represents the perplexity values. Memory is reported in GiB. Speed represents the number of iterations per second. The bold numbers represent the top results, whereas the underlined numbers are the second-best ones.", "description": "Table 3 presents a comprehensive evaluation of the lossy NeuZip compression technique on various neural network models and tasks.  It compares the performance (perplexity), memory usage (in GiB), and training speed (iterations per second) of lossy NeuZip against several baseline methods, including standard models and various quantization techniques (INT8, FP4, NF4).  The table shows the perplexity scores, memory requirements, and iteration speeds achieved by each method, enabling a detailed comparison of the trade-off between memory efficiency and model accuracy.  The bold values indicate the best results for each model and task, while the underlined numbers highlight the second-best performing methods.", "section": "3.3 Lossy Compression for Inference"}, {"content": "| Name | T5 1B PPL | T5 1B Mem | T5 1B Speed | T5 3B PPL | T5 3B Mem | T5 3B Speed | T5 11B PPL | T5 11B Mem | T5 11B Speed |\n|---|---|---|---|---|---|---|---|---|---| \n| Vanilla | **2.614** | 1.37 | **23.73** | **2.571** | 5.31 | **19.86** | **2.568** | 21.06 | **6.20** |\n| Quant INT8 | 2.615 | 1.28 | 4.24 | **2.573** | 4.94 | 4.28 | **2.569** | 19.59 | 2.58 |\n| Quant NF4 | 2.632 | 1.08 | 11.64 | 2.588 | 4.12 | 11.82 | 2.579 | 16.28 | 4.48 |\n| Quant FP4 | 2.646 | 1.08 | 11.92 | 2.594 | 4.12 | **11.99** | 2.585 | 16.28 | **4.59** |\n| Quant FP4<sup>2</sup> | 2.646 | 1.05 | 10.39 | 2.594 | 4.03 | 9.72 | 2.585 | 15.93 | 4.52 |\n| Quant NF4<sup>2</sup> | 2.632 | 1.05 | 10.39 | 2.587 | 4.03 | 9.96 | 2.579 | 15.93 | 4.39 |\n| NeuZip 0-bit | 2.731 | **0.40** | 11.82 | 2.668 | **1.41** | 8.70 | 2.651 | **5.35** | 3.24 |\n| NeuZip 1-bit | 2.641 | **0.48** | 11.68 | 2.591 | **1.78** | 8.61 | 2.581 | **6.65** | 3.21 |\n| NeuZip 3-bit | **2.614** | 0.66 | **11.99** | 2.574 | 2.42 | 8.60 | **2.569** | 9.27 | 3.19 |\n| NeuZip 7-bit (lossless) | **2.614** | 0.99 | 11.55 | **2.571** | 3.73 | 8.77 | **2.568** | 14.46 | 3.23 |", "caption": "(a) Evaluating decoder-only models on the language modeling task. Here, the perplexities are adjusted to word level to compare across different tokenizations.", "description": "This table presents the results of evaluating decoder-only language models on a language modeling task.  The models are compared across three metrics: perplexity (PPL), memory usage (in GiB), and training speed (iterations per second).  Perplexity scores are adjusted to the word level to allow for fair comparison across models with different tokenization schemes.  The table includes results for a vanilla (uncompressed) model, several quantization methods (INT8, FP4, NF4, FP42, NF42), and different variations of the NeuZip algorithm (0-bit, 1-bit, 3-bit, and 7-bit (lossless)). This allows for a comprehensive comparison of model performance, efficiency, and memory footprint.", "section": "3.3 Lossy NeuZip for Inference"}, {"content": "| Name | Block 32 | Block 32 | Block 64 | Block 64 | Block 128 | Block 128 | Block 256 | Block 256 | Block 512 | Block 512 |\n|---|---|---|---|---|---|---|---|---|---|---|\n|  | PPL | Mem | PPL | Mem | PPL | Mem | PPL | Mem | PPL | Mem |\n| NeuZip 0-bit | 6.341 | 35.7 | 6.694 | 34.6 | 6.853 | 34.2 | 7.639 | 33.8 | 7.104 | 33.5 |\n| NeuZip 1-bit | - | OOM | 4.611 | 42.7 | 4.662 | 42.2 | 4.640 | 41.8 | 4.649 | 41.4 |", "caption": "(b) Evaluating encoder\u2013decoder models on the language modeling task. Since all models use the same tokenizer, we reported perplexities at the token level for simplicity.", "description": "This table presents the results of evaluating encoder-decoder models (T5 models of various sizes) on a language modeling task.  Because all models used the same tokenizer, perplexity is reported at the token level for simpler comparison and easier interpretation of the results.  The table likely shows metrics like perplexity (PPL), memory usage (Mem), and training speed (Speed) for different model sizes and/or compression techniques.  The focus is on comparing the impact of different methods on efficiency and accuracy.", "section": "3.2 Lossless NeuZip for Fine-Tuning"}]